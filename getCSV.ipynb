{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract weights without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_12\n",
      "dense_12\n",
      "bias:0\n",
      "kernel:0\n",
      "dense_13\n",
      "dense_13\n",
      "bias:0\n",
      "kernel:0\n",
      "dense_14\n",
      "dense_14\n",
      "bias:0\n",
      "kernel:0\n",
      "Total length: \n",
      "17801\n"
     ]
    }
   ],
   "source": [
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Read H5 file\n",
    "f = h5.File(\"weights_files/features_fit_transform_model_weights.h5\", \"r\")\n",
    "# Get and print list of datasets within the H5 file\n",
    "datasetNames = list(f.keys())\n",
    "\n",
    "content = []\n",
    "\n",
    "for k1 in f:\n",
    "    print(k1)\n",
    "    for k2 in f[k1].keys():\n",
    "        print(k2)\n",
    "        for k3 in f[k1][k2].keys():  \n",
    "            print(k3)\n",
    "            \n",
    "            data = list(f[k1][k2][k3])\n",
    "            length = len(data)\n",
    "\n",
    "\n",
    "            if (isinstance(data[0], np.float32)):\n",
    "                for i in range(length):\n",
    "                    content.append(data[i]) \n",
    "            else:\n",
    "                for i in range(length):\n",
    "                    sub_length = len(data[i])\n",
    "                    sub_data = data[i]\n",
    "                    for j in range(sub_length):\n",
    "                        content.append(sub_data[j])\n",
    "                    \n",
    "                \n",
    "print(\"Total length: \")\n",
    "print(len(content))\n",
    "\n",
    "                \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"mlpv5.csv\", content, '%s', newline=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract weights with quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant_dense\n",
      "  dense\n",
      "    bias:0\n",
      "    kernel:0\n",
      "  quant_dense\n",
      "    kernel_max:0\n",
      "    kernel_min:0\n",
      "    optimizer_step:0\n",
      "    post_activation_max:0\n",
      "    post_activation_min:0\n",
      "quant_dense_1\n",
      "  dense_1\n",
      "    bias:0\n",
      "    kernel:0\n",
      "  quant_dense_1\n",
      "    kernel_max:0\n",
      "    kernel_min:0\n",
      "    optimizer_step:0\n",
      "    post_activation_max:0\n",
      "    post_activation_min:0\n",
      "quant_dense_2\n",
      "  dense_2\n",
      "    bias:0\n",
      "    kernel:0\n",
      "  quant_dense_2\n",
      "    kernel_max:0\n",
      "    kernel_min:0\n",
      "    optimizer_step:0\n",
      "    pre_activation_max:0\n",
      "    pre_activation_min:0\n",
      "quantize_layer\n",
      "  quantize_layer\n",
      "    optimizer_step:0\n",
      "    quantize_layer_max:0\n",
      "    quantize_layer_min:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:26: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:28: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:30: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:32: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: \n",
      "17801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take quantization into consideration\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read H5 file\n",
    "f = h5.File(\"weights_files/quantized_feature_fit_transform_model_weights.h5\", \"r\")\n",
    "# Get and print list of datasets within the H5 file\n",
    "datasetNames = list(f.keys())\n",
    "\n",
    "content = []\n",
    "bias_list = []\n",
    "weights_list = []\n",
    "weights_max = []\n",
    "weights_min = []\n",
    "\n",
    "for k1 in f:\n",
    "    print(k1)\n",
    "    for k2 in f[k1].keys():\n",
    "        print(\"  \"+k2)\n",
    "        for k3 in f[k1][k2].keys(): \n",
    "            print(\"    \"+k3)\n",
    "            if \"bias\" in k3:\n",
    "                bias_list.append(f[k1][k2][k3].value)\n",
    "            if \"kernel:0\" in k3:\n",
    "                weights_list.append(f[k1][k2][k3].value)\n",
    "            if \"kernel_max:0\" in k3:\n",
    "                weights_max.append(f[k1][k2][k3].value)\n",
    "            if \"kernel_min:0\" in k3:\n",
    "                weights_min.append(f[k1][k2][k3].value)\n",
    "            \n",
    "            \n",
    "for i in range(len(weights_max)):\n",
    "    S = (weights_max[i]-weights_min[i])/255.0\n",
    "    Z = 127 - (weights_max[i]/S)\n",
    "    for j in range(len(bias_list[i])):\n",
    "        bias_list[i][j] = (bias_list[i][j]/S) + Z\n",
    "        content.append(bias_list[i][j])\n",
    "        \n",
    "    for j in range(len(weights_list[i])):\n",
    "        for k in range(len(weights_list[i][j])):\n",
    "            weights_list[i][j][k] = (weights_list[i][j][k]/S) + Z\n",
    "            content.append(weights_list[i][j][k])\n",
    "            \n",
    "print(\"Total length: \")\n",
    "print(len(content))\n",
    "\n",
    "len(bias_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"mlpv5_1_quantized.csv\", content, '%s', newline=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
