{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'weights_files/fit_transform_model_weights_Tuesday_week_13.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ee2676762be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Read H5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights_files/fit_transform_model_weights_Tuesday_week_13.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Get and print list of datasets within the H5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdatasetNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'weights_files/fit_transform_model_weights_Tuesday_week_13.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Read H5 file\n",
    "f = h5.File(\"weights_files/fit_transform_model_weights_Tuesday_week_13.h5\", \"r\")\n",
    "# Get and print list of datasets within the H5 file\n",
    "datasetNames = list(f.keys())\n",
    "\n",
    "content = []\n",
    "\n",
    "for k1 in f:\n",
    "    print(k1)\n",
    "    for k2 in f[k1].keys():\n",
    "        print(k2)\n",
    "        for k3 in f[k1][k2].keys():  \n",
    "            print(k3)\n",
    "            \n",
    "            data = list(f[k1][k2][k3])\n",
    "            length = len(data)\n",
    "            print(f[k1][k2][k3].value.shape)\n",
    "\n",
    "            if (isinstance(data[0], np.float32)):\n",
    "                for i in range(length):\n",
    "                    content.append(data[i]) \n",
    "            else:\n",
    "                for i in range(length):\n",
    "                    sub_length = len(data[i])\n",
    "                    sub_data = data[i]\n",
    "                    for j in range(sub_length):\n",
    "#                         print(sub_data[j])\n",
    "                        content.append(sub_data[j])\n",
    "                    \n",
    "                \n",
    "print(\"Total length: \")\n",
    "print(len(content))\n",
    "print(type(content[1]))\n",
    "\n",
    "                \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"mlpv4_4_stdscaler.csv\", content, '%s', newline=',')\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv('mlpv2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant_dense_4\n",
      "  dense_4\n",
      "    bias:0\n",
      "    kernel:0\n",
      "  quant_dense_4\n",
      "    kernel_max:0\n",
      "    kernel_min:0\n",
      "    optimizer_step:0\n",
      "    post_activation_max:0\n",
      "    post_activation_min:0\n",
      "quant_dense_5\n",
      "  dense_5\n",
      "    bias:0\n",
      "    kernel:0\n",
      "  quant_dense_5\n",
      "    kernel_max:0\n",
      "    kernel_min:0\n",
      "    optimizer_step:0\n",
      "    post_activation_max:0\n",
      "    post_activation_min:0\n",
      "quant_dense_6\n",
      "  dense_6\n",
      "    bias:0\n",
      "    kernel:0\n",
      "  quant_dense_6\n",
      "    kernel_max:0\n",
      "    kernel_min:0\n",
      "    optimizer_step:0\n",
      "    post_activation_max:0\n",
      "    post_activation_min:0\n",
      "quant_dense_7\n",
      "  dense_7\n",
      "    bias:0\n",
      "    kernel:0\n",
      "  quant_dense_7\n",
      "    kernel_max:0\n",
      "    kernel_min:0\n",
      "    optimizer_step:0\n",
      "    pre_activation_max:0\n",
      "    pre_activation_min:0\n",
      "quantize_layer_1\n",
      "  quantize_layer_1\n",
      "    optimizer_step:0\n",
      "    quantize_layer_1_max:0\n",
      "    quantize_layer_1_min:0\n",
      "Total length: \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Take quantization into consideration\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read H5 file\n",
    "f = h5.File(\"weights_files/MLPmodel_Week13_NoFeature_QAware.h5\", \"r\")\n",
    "# Get and print list of datasets within the H5 file\n",
    "datasetNames = list(f.keys())\n",
    "\n",
    "content = []\n",
    "\n",
    "for k1 in f:\n",
    "    print(k1)\n",
    "    for k2 in f[k1].keys():\n",
    "        print(\"  \"+k2)\n",
    "        for k3 in f[k1][k2].keys(): \n",
    "            print(\"    \"+k3)\n",
    "#             print(f[k1][k2][k3].value)\n",
    "            \n",
    "#             data = list(f[k1][k2][k3])\n",
    "#             length = len(data)\n",
    "#             print(f[k1][k2][k3].value.shape)\n",
    "            \n",
    "#             if (isinstance(data[0], np.float32)):\n",
    "#                 for i in range(length):\n",
    "#                     content.append(data[i]) \n",
    "#             else:\n",
    "#                 for i in range(length):\n",
    "#                     sub_length = len(data[i])\n",
    "#                     sub_data = data[i]\n",
    "#                     for j in range(sub_length):\n",
    "#                         content.append(sub_data[j])\n",
    "       \n",
    "print(\"Total length: \")\n",
    "print(len(content))\n",
    "\n",
    "data = {'params': content}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take quantization into consideration\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read H5 file\n",
    "f = h5.File(\"weights_files/MLPmodel_Week13_NoFeature_QAware.h5\", \"r\")\n",
    "# Get and print list of datasets within the H5 file\n",
    "datasetNames = list(f.keys())\n",
    "\n",
    "content = []\n",
    "bias_list = []\n",
    "\n",
    "for k1 in f:\n",
    "    print(k1)\n",
    "    for k2 in f[k1].keys():\n",
    "        print(\"  \"+k2)\n",
    "        for k3 in f[k1][k2].keys(): \n",
    "            print(\"    \"+k3)\n",
    "            if \"bias\" in k3:\n",
    "                bias_list.append(f[k1][k2][k3])\n",
    "            \n",
    "                \n",
    "print(\"Total length: \")\n",
    "print(len(content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
